{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "n_data0 = torch.ones(100,2)\n",
    "n_data1 = torch.ones(100,2)\n",
    "n_data2 = torch.ones(100,2)\n",
    "n_data3 = torch.ones(100,2)\n",
    "n_data0[:,0] = 2\n",
    "n_data0[:,1] = 2\n",
    "n_data1[:,0] = -2\n",
    "n_data1[:,1] = 2\n",
    "n_data2[:,0] = -2\n",
    "n_data2[:,1] = -2\n",
    "n_data3[:,0] = 2\n",
    "n_data3[:,1] = -2\n",
    "\n",
    "x0 = torch.normal(n_data0,1)\n",
    "y0 = torch.zeros(100)\n",
    "x1 = torch.normal(n_data1,1)\n",
    "y1 = torch.ones(100)\n",
    "x2 = torch.normal(n_data2,1)\n",
    "y2 = torch.ones(100) * 2\n",
    "x3 = torch.normal(n_data3,1)\n",
    "y3 = torch.ones(100) * 3\n",
    "\n",
    "\n",
    "x = torch.cat((x0,x1,x2,x3),0).type(torch.FloatTensor)\n",
    "y = torch.cat((y0,y1,y2,y3)).type(torch.LongTensor)\n",
    "\n",
    "x,y = Variable(x),Variable(y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5782,  1.9733],\n",
      "        [ 1.6304,  1.3470],\n",
      "        [ 2.2527,  1.7385],\n",
      "        [ 2.6671,  1.8933],\n",
      "        [ 1.5355,  0.9263],\n",
      "        [ 2.5835,  3.6458],\n",
      "        [ 0.6335,  2.1339],\n",
      "        [ 4.2071,  1.0152],\n",
      "        [ 1.7378,  1.9698],\n",
      "        [ 1.7174,  1.3269],\n",
      "        [ 1.1527, -0.4738],\n",
      "        [ 2.5836,  1.3607],\n",
      "        [ 1.1286,  1.8424],\n",
      "        [ 1.4861,  2.4009],\n",
      "        [ 3.9961,  1.6146],\n",
      "        [ 3.0012,  3.0528],\n",
      "        [ 1.7875,  2.3576],\n",
      "        [ 1.7529,  0.5185],\n",
      "        [ 2.0024,  2.6000],\n",
      "        [ 3.0415,  3.2715],\n",
      "        [ 2.1986,  3.0465],\n",
      "        [ 1.6474,  0.8613],\n",
      "        [ 1.3286,  1.1349],\n",
      "        [ 2.1822,  2.0085],\n",
      "        [-0.7102,  3.9474],\n",
      "        [ 3.7662,  3.4044],\n",
      "        [ 3.8342,  3.3204],\n",
      "        [ 2.6532,  2.0025],\n",
      "        [ 0.8261,  1.3637],\n",
      "        [ 3.2602,  0.7159],\n",
      "        [ 3.0103,  1.6008],\n",
      "        [ 1.8387,  3.0127],\n",
      "        [ 2.9626,  1.7865],\n",
      "        [ 2.7591,  3.5471],\n",
      "        [ 1.9123,  2.2059],\n",
      "        [ 1.5944,  3.3481],\n",
      "        [ 0.5340,  1.2213],\n",
      "        [ 1.3653,  2.1797],\n",
      "        [ 1.7065,  3.2843],\n",
      "        [ 3.1298,  3.8168],\n",
      "        [ 0.8274,  2.8644],\n",
      "        [ 1.7300,  2.4052],\n",
      "        [ 2.3158,  1.7243],\n",
      "        [ 2.6317,  2.2806],\n",
      "        [ 2.5477,  1.5247],\n",
      "        [ 2.5920,  1.8362],\n",
      "        [ 1.3513,  0.8322],\n",
      "        [ 0.8585,  3.3093],\n",
      "        [ 2.2465,  1.6173],\n",
      "        [ 2.5947,  2.0360],\n",
      "        [ 2.4675,  1.7443],\n",
      "        [ 2.9450,  1.5392],\n",
      "        [ 1.9164,  0.5361],\n",
      "        [ 1.6018,  1.3939],\n",
      "        [ 1.2943,  4.1177],\n",
      "        [ 0.7971,  1.8428],\n",
      "        [ 1.7481,  0.2038],\n",
      "        [ 3.7848,  2.8720],\n",
      "        [ 0.7362, -0.0107],\n",
      "        [ 3.1399,  3.7177],\n",
      "        [ 3.2390,  1.1969],\n",
      "        [ 1.8174,  1.7944],\n",
      "        [ 2.1524,  1.9245],\n",
      "        [ 2.4484,  1.3482],\n",
      "        [ 1.7966,  1.4852],\n",
      "        [ 3.2040,  1.7948],\n",
      "        [ 0.2862,  1.9591],\n",
      "        [ 0.2693,  1.4321],\n",
      "        [ 0.8942,  1.3751],\n",
      "        [ 2.3661,  2.2651],\n",
      "        [ 0.9354,  1.9712],\n",
      "        [ 1.5240,  2.7577],\n",
      "        [ 0.8245,  2.1450],\n",
      "        [ 3.3589, -0.6661],\n",
      "        [ 0.1461,  1.6863],\n",
      "        [ 2.1783, -0.3510],\n",
      "        [ 4.1350,  1.9756],\n",
      "        [ 2.2416,  2.0172],\n",
      "        [ 2.6357,  1.2936],\n",
      "        [ 2.3838,  2.7177],\n",
      "        [ 1.1954,  2.3380],\n",
      "        [-0.0149,  2.1602],\n",
      "        [ 2.7541,  0.9386],\n",
      "        [ 2.9754,  0.1792],\n",
      "        [ 1.6207,  1.7770],\n",
      "        [ 2.6486,  2.8555],\n",
      "        [ 2.0019,  2.2193],\n",
      "        [ 3.9902, -1.1526],\n",
      "        [ 0.8560,  4.2296],\n",
      "        [ 1.6229,  0.9617],\n",
      "        [ 1.8656,  4.1354],\n",
      "        [-0.1169,  2.7275],\n",
      "        [ 1.5703,  1.9183],\n",
      "        [ 1.4107,  1.7233],\n",
      "        [ 0.7847,  2.0113],\n",
      "        [ 4.3176,  0.1275],\n",
      "        [ 2.3161,  1.7118],\n",
      "        [ 2.4770,  1.7516],\n",
      "        [ 2.5753,  0.3274],\n",
      "        [ 1.3841,  2.8178],\n",
      "        [-1.8243,  1.6014],\n",
      "        [-0.2392,  3.3009],\n",
      "        [-1.5442,  3.3503],\n",
      "        [-0.3034,  1.7181],\n",
      "        [-2.9589,  2.4310],\n",
      "        [-1.1101,  0.8708],\n",
      "        [-1.8375,  1.7366],\n",
      "        [-2.5599,  2.4870],\n",
      "        [-2.5155,  2.5349],\n",
      "        [-1.5137,  2.3224],\n",
      "        [-2.8664,  3.0063],\n",
      "        [-2.4838,  2.6126],\n",
      "        [-0.0531,  2.3266],\n",
      "        [-2.9737,  1.9192],\n",
      "        [-1.3233,  0.9669],\n",
      "        [-1.4781,  1.5772],\n",
      "        [-2.0602,  1.7038],\n",
      "        [-2.4287,  2.2419],\n",
      "        [-1.2525,  4.0057],\n",
      "        [-1.9108,  2.7869],\n",
      "        [-2.3522,  1.5259],\n",
      "        [-2.5054,  2.8052],\n",
      "        [-3.4372,  0.5649],\n",
      "        [-2.0194,  1.5894],\n",
      "        [-0.8277,  2.2942],\n",
      "        [-2.2690,  1.8570],\n",
      "        [-3.1975,  2.3095],\n",
      "        [-0.3953,  0.9279],\n",
      "        [-1.9438,  2.4751],\n",
      "        [-1.4485,  2.2402],\n",
      "        [-3.1714,  1.1613],\n",
      "        [-3.6501,  3.6937],\n",
      "        [-2.4776,  4.2017],\n",
      "        [-1.4569,  2.3090],\n",
      "        [-2.0593, -0.0900],\n",
      "        [-0.2444,  2.3281],\n",
      "        [-2.2705,  3.7380],\n",
      "        [-2.0502,  3.3339],\n",
      "        [ 0.7197,  1.5661],\n",
      "        [-2.0704,  1.8433],\n",
      "        [-2.9326,  1.0396],\n",
      "        [-2.4404,  3.4441],\n",
      "        [-3.0413,  1.4696],\n",
      "        [-2.3162,  2.2790],\n",
      "        [-1.8362,  1.4221],\n",
      "        [-0.9170,  0.2027],\n",
      "        [-3.0375,  2.8197],\n",
      "        [-1.4985,  2.1889],\n",
      "        [-2.9775,  3.2375],\n",
      "        [-1.0497,  1.7842],\n",
      "        [-2.9051,  2.2035],\n",
      "        [-0.8260,  2.6103],\n",
      "        [-0.9815,  2.6090],\n",
      "        [-1.3901,  3.6252],\n",
      "        [-2.1856,  0.6301],\n",
      "        [-3.0483,  1.8438],\n",
      "        [ 0.3525,  1.8076],\n",
      "        [-1.4635,  0.8915],\n",
      "        [-1.9257,  2.0143],\n",
      "        [-1.6348,  2.4863],\n",
      "        [-2.1373,  0.7473],\n",
      "        [-1.3195,  1.8277],\n",
      "        [-1.2222,  2.9302],\n",
      "        [-3.5639,  1.6255],\n",
      "        [-2.3856,  2.1924],\n",
      "        [-1.8763,  1.7241],\n",
      "        [-1.3068,  2.5185],\n",
      "        [-4.0220,  1.0316],\n",
      "        [-2.0615,  1.8376],\n",
      "        [-0.8603,  2.7514],\n",
      "        [-1.0648,  1.2556],\n",
      "        [-1.3973,  4.0704],\n",
      "        [-0.4367,  0.7845],\n",
      "        [-3.1102,  1.5596],\n",
      "        [-3.3303,  2.1573],\n",
      "        [-1.6333,  1.8123],\n",
      "        [-1.4690,  3.1777],\n",
      "        [-2.8566,  3.7851],\n",
      "        [-1.6698,  1.2728],\n",
      "        [-1.0308,  2.9792],\n",
      "        [-1.2741,  1.1396],\n",
      "        [-1.8884,  2.8020],\n",
      "        [-2.7238,  2.7116],\n",
      "        [-2.0499,  2.2121],\n",
      "        [-0.6163,  2.0031],\n",
      "        [-0.2532,  1.0939],\n",
      "        [-2.9096,  3.1369],\n",
      "        [-2.2384,  0.5483],\n",
      "        [-0.1978,  2.2844],\n",
      "        [-1.1998,  2.2081],\n",
      "        [-2.2786,  1.1691],\n",
      "        [-0.8473,  2.3832],\n",
      "        [-2.0894,  2.5696],\n",
      "        [-2.0037,  0.4470],\n",
      "        [-1.7875,  1.8927],\n",
      "        [-1.6746,  1.0990],\n",
      "        [-2.3517,  2.4895],\n",
      "        [-3.4011,  0.4068],\n",
      "        [-0.9895,  1.6271],\n",
      "        [-2.3284,  2.9881],\n",
      "        [-2.0011, -2.1837],\n",
      "        [-2.2743, -2.5678],\n",
      "        [-1.1271, -2.3619],\n",
      "        [-1.2591, -0.8272],\n",
      "        [-0.4946, -2.3585],\n",
      "        [-2.3475, -0.6382],\n",
      "        [-1.8766, -1.3365],\n",
      "        [-1.8986, -0.8369],\n",
      "        [-2.6548, -2.0277],\n",
      "        [-0.6758, -2.6676],\n",
      "        [-1.6190, -3.7970],\n",
      "        [-2.9411, -1.8482],\n",
      "        [-2.5022, -0.2468],\n",
      "        [-2.0267, -0.0966],\n",
      "        [-1.3600, -2.8983],\n",
      "        [-1.8675, -1.2768],\n",
      "        [-1.4977, -1.0511],\n",
      "        [-0.5567, -0.8619],\n",
      "        [-0.6501, -2.4051],\n",
      "        [-0.4731, -1.1687],\n",
      "        [-0.3657, -0.9851],\n",
      "        [-1.2088, -2.5237],\n",
      "        [-1.4026, -2.4896],\n",
      "        [-0.5417, -1.5197],\n",
      "        [-1.0563, -2.9104],\n",
      "        [-2.5940, -0.4952],\n",
      "        [-0.0952, -2.5925],\n",
      "        [-2.9222, -1.5632],\n",
      "        [-2.9249, -2.6137],\n",
      "        [-0.7379, -2.7007],\n",
      "        [-2.1099, -1.6974],\n",
      "        [-1.9717, -0.7930],\n",
      "        [-2.3979, -2.3056],\n",
      "        [-2.4618, -1.7678],\n",
      "        [-2.1604, -4.0340],\n",
      "        [-1.4693, -2.4055],\n",
      "        [-0.9677, -2.1326],\n",
      "        [-2.5303, -1.4387],\n",
      "        [-3.2349, -2.5805],\n",
      "        [-4.0420, -1.7670],\n",
      "        [-2.6752, -0.6535],\n",
      "        [-3.0765, -1.5076],\n",
      "        [-2.8475, -2.8516],\n",
      "        [-2.0608, -2.7171],\n",
      "        [-1.9612, -2.6075],\n",
      "        [-0.1219, -2.6804],\n",
      "        [-2.9956, -1.2130],\n",
      "        [-2.2123, -2.7906],\n",
      "        [-2.8097, -1.2110],\n",
      "        [-2.1749, -1.1018],\n",
      "        [-2.5619, -3.2295],\n",
      "        [-0.2731, -4.2369],\n",
      "        [-0.8676, -3.2308],\n",
      "        [ 0.5369, -3.4424],\n",
      "        [-1.3459, -2.8446],\n",
      "        [-1.9869, -2.1711],\n",
      "        [-3.4394, -1.9835],\n",
      "        [-1.0232, -2.9281],\n",
      "        [-2.6715, -0.4622],\n",
      "        [-2.3071, -1.7830],\n",
      "        [-1.7112, -0.3354],\n",
      "        [ 0.0458, -1.9399],\n",
      "        [-2.6020, -3.0373],\n",
      "        [-1.5920, -2.3429],\n",
      "        [-2.1878, -3.7269],\n",
      "        [-1.8634, -1.8026],\n",
      "        [-2.5696, -2.0804],\n",
      "        [-3.0970, -1.7427],\n",
      "        [-4.0055, -1.9464],\n",
      "        [-1.3179, -2.1821],\n",
      "        [-1.9821, -1.7224],\n",
      "        [-1.3760, -2.2226],\n",
      "        [-2.6518, -0.9725],\n",
      "        [-2.9439, -3.0091],\n",
      "        [-0.4954, -1.9043],\n",
      "        [-1.3948, -1.8200],\n",
      "        [-0.9650, -1.8903],\n",
      "        [-1.4603, -2.2058],\n",
      "        [-2.2737, -2.8799],\n",
      "        [-0.7851, -1.8187],\n",
      "        [-3.6682, -2.3132],\n",
      "        [-2.1451, -1.1044],\n",
      "        [-1.7249, -0.6803],\n",
      "        [-3.4357, -2.8581],\n",
      "        [-4.2340, -2.8606],\n",
      "        [-1.6062, -2.2378],\n",
      "        [-1.5914, -3.0557],\n",
      "        [-1.2439, -1.9837],\n",
      "        [-3.1186, -0.4268],\n",
      "        [-3.5794, -2.1373],\n",
      "        [-1.1235, -2.9418],\n",
      "        [-1.0369, -3.6931],\n",
      "        [-2.2210, -2.2424],\n",
      "        [-2.2873, -3.5023],\n",
      "        [-1.7217, -1.0320],\n",
      "        [-1.3568, -2.9000],\n",
      "        [-2.3703, -2.0350],\n",
      "        [-2.5039, -3.0298],\n",
      "        [-2.2037, -2.9501],\n",
      "        [-1.5722, -3.9589],\n",
      "        [ 2.5363, -1.1910],\n",
      "        [ 1.1799, -0.9676],\n",
      "        [ 0.3067, -1.5660],\n",
      "        [ 4.6110, -3.4786],\n",
      "        [ 2.0336, -0.8720],\n",
      "        [ 2.1393, -0.5298],\n",
      "        [ 2.2168, -3.2122],\n",
      "        [ 2.0724, -0.5252],\n",
      "        [ 2.3800, -0.8298],\n",
      "        [-0.0433, -1.5160],\n",
      "        [ 3.2265, -1.0279],\n",
      "        [ 2.5113, -3.5993],\n",
      "        [ 1.7138, -0.4076],\n",
      "        [ 2.6166, -3.7092],\n",
      "        [ 2.8140, -1.6203],\n",
      "        [ 3.5635, -1.2053],\n",
      "        [ 2.8323, -3.3850],\n",
      "        [ 1.6058, -4.0567],\n",
      "        [ 1.1686, -1.3977],\n",
      "        [ 1.6180, -2.8317],\n",
      "        [ 1.5930, -1.2359],\n",
      "        [ 2.0708, -3.2441],\n",
      "        [ 2.3529, -0.9012],\n",
      "        [ 3.3859, -3.8718],\n",
      "        [ 1.2267, -2.2644],\n",
      "        [ 1.2504, -4.1075],\n",
      "        [ 3.1092, -2.2271],\n",
      "        [ 1.9545, -1.8591],\n",
      "        [ 1.5401, -2.0815],\n",
      "        [ 2.3936, -2.4109],\n",
      "        [ 3.7190, -1.7159],\n",
      "        [ 1.1506, -1.4525],\n",
      "        [ 2.6896, -1.3843],\n",
      "        [ 3.0153, -3.6062],\n",
      "        [ 3.0308, -1.8598],\n",
      "        [ 3.7596, -3.0541],\n",
      "        [ 2.1090, -2.6461],\n",
      "        [ 2.2358, -2.1525],\n",
      "        [ 3.7130, -1.6115],\n",
      "        [ 2.0524, -2.2622],\n",
      "        [ 1.6442, -1.6836],\n",
      "        [ 1.9701, -2.0276],\n",
      "        [ 2.9159, -1.9785],\n",
      "        [ 1.0972, -0.5334],\n",
      "        [ 1.8249, -0.2948],\n",
      "        [ 0.5638, -2.6035],\n",
      "        [ 2.7595,  0.0857],\n",
      "        [ 1.2053, -3.6742],\n",
      "        [ 1.9790, -2.9834],\n",
      "        [ 2.1935, -2.3352],\n",
      "        [ 1.1634, -3.4450],\n",
      "        [ 2.3417, -2.6034],\n",
      "        [ 1.9898, -2.3218],\n",
      "        [ 3.5974, -2.0285],\n",
      "        [ 0.2693, -2.9770],\n",
      "        [ 2.2408, -2.1269],\n",
      "        [ 3.3320, -1.7300],\n",
      "        [ 1.8524, -2.6161],\n",
      "        [ 3.0650, -1.6596],\n",
      "        [ 4.1249, -2.2858],\n",
      "        [ 2.3267, -0.0499],\n",
      "        [-0.0674, -1.4749],\n",
      "        [ 2.9084, -1.8791],\n",
      "        [ 2.4746, -1.4118],\n",
      "        [ 1.5124, -0.6719],\n",
      "        [ 1.1167, -2.5368],\n",
      "        [ 2.3882, -1.7446],\n",
      "        [ 1.5821, -3.2986],\n",
      "        [ 1.3892, -1.7766],\n",
      "        [ 1.9964, -1.3958],\n",
      "        [ 1.5648, -2.3407],\n",
      "        [ 1.5585, -1.0914],\n",
      "        [ 1.8222, -3.4784],\n",
      "        [ 1.6326, -1.3966],\n",
      "        [ 2.3170, -1.8781],\n",
      "        [ 1.9847, -2.8008],\n",
      "        [ 1.4246, -2.1632],\n",
      "        [ 2.0571, -0.9731],\n",
      "        [ 2.2316, -1.0089],\n",
      "        [ 1.3864, -2.2158],\n",
      "        [ 1.0173, -1.7928],\n",
      "        [ 3.0131, -2.3465],\n",
      "        [ 0.1016, -2.7239],\n",
      "        [ 1.8644, -1.2522],\n",
      "        [ 2.0740, -1.8862],\n",
      "        [ 2.0929, -2.2766],\n",
      "        [ 1.3730, -1.7164],\n",
      "        [ 2.5870, -0.7104],\n",
      "        [ 0.9371, -1.4919],\n",
      "        [ 1.2884, -1.6026],\n",
      "        [ 2.3922, -2.4239],\n",
      "        [ 2.4391, -2.0359],\n",
      "        [ 2.4336, -1.5321],\n",
      "        [ 3.1896, -1.5183],\n",
      "        [ 2.7378, -2.7829],\n",
      "        [ 0.4780, -0.4520],\n",
      "        [ 1.4053, -1.3549],\n",
      "        [ 2.1243, -1.9039],\n",
      "        [ 2.0136, -4.5684],\n",
      "        [ 1.4072, -1.1637]])\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self,n_input,n_hidden,n_output):\n",
    "        super(Net,self).__init__()\n",
    "        self.hidden1 = torch.nn.Linear(n_input,n_hidden)\n",
    "        self.hidden2 = torch.nn.Linear(n_hidden,n_hidden)\n",
    "        self.predict = torch.nn.Linear(n_hidden,n_output)\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = self.hidden1(input)\n",
    "        out = F.relu(out)\n",
    "        out = self.hidden2(out)\n",
    "        out = F.sigmoid(out)\n",
    "        out = self.predict(out)\n",
    "        out = F.softmax(out)\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "def CrossEntropyLoss(out,y):\n",
    "    softmax = torch.exp(out)/torch.sum(torch.exp(out))\n",
    "    logsoftmax = torch.log(softmax)\n",
    "    nllloss = -torch.sum(y*logsoftmax)/y.shape[0]\n",
    "    return nllloss + logsoftmax"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "net = Net(2,20,4)\n",
    "optimizer = torch.optim.NAdam(net.parameters(),lr=0.02)\n",
    "loss_func = torch.nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7721, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7729, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7737, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7728, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7729, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7727, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7731, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7727, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7725, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7729, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7751, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7752, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7729, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7729, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7738, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7725, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7725, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7729, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7732, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7743, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7732, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7723, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7728, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7732, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7725, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7722, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7731, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7741, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7736, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7728, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7725, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7720, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7729, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7735, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7729, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7744, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7748, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7750, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7748, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7738, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7728, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7725, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7733, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7738, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7725, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7729, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7736, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7724, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7724, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7729, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7738, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7718, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7718, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7719, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7721, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7731, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7741, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7733, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7726, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7730, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7721, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7732, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7728, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7729, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7732, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7730, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7724, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7725, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7726, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7722, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7728, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7742, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7750, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7736, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7735, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7734, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7731, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7726, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7750, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7739, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7731, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7728, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7728, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7735, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7726, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7728, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7727, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7726, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7730, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7735, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7740, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7737, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7732, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7732, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7727, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7736, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7735, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7731, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7733, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7721, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\MySoftware\\anaconda3\\envs\\MyPytorch\\lib\\site-packages\\torch\\nn\\functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "D:\\MySoftware\\anaconda3\\envs\\MyPytorch\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for t in range(100):\n",
    "    out = net(x)\n",
    "    # print(out)\n",
    "    loss = F.cross_entropy(out,y)\n",
    "    # loss2 = CrossEntropyLoss(out,y)\n",
    "    # print(loss2 - loss1)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss)\n",
    "    # print(torch.max(F.softmax(out),1)[1])\n",
    "    # print(loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"./model\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}